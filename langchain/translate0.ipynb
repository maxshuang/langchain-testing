{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f042f59a-127a-4334-a7ab-5b6029664a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Step 1: Define your LMStudio local server's endpoint\n",
    "local_llm_url = \"http://172.20.10.9:1234\"  # Replace with your actual IP and port\n",
    "\n",
    "# API endpoints for the LMStudio server\n",
    "chat_endpoint = f\"{local_llm_url}/v1/chat/completions\"\n",
    "completion_endpoint = f\"{local_llm_url}/v1/completions\"\n",
    "\n",
    "def get_completion(prompt):\n",
    "    try:\n",
    "        # Send the prompt to the local LLM via an HTTP POST request\n",
    "        response = requests.post(\n",
    "            completion_endpoint,\n",
    "            json={\"prompt\": prompt, \"max_tokens\": 100}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully connected to the local LLM.\")\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"text\", \"No response\")\n",
    "        else:\n",
    "            print(f\"Failed to connect to the local LLM: {response.status_code}, {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the local LLM: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a205a284-5f66-4a19-b316-81b463722d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(messages):\n",
    "    try:\n",
    "        # Send a chat-style request to the local LLM\n",
    "        response = requests.post(\n",
    "            chat_endpoint,\n",
    "            json={\"messages\": messages, \"max_tokens\": 100}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully connected to the local LLM for chat.\")\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "        else:\n",
    "            print(f\"Failed to connect to the local LLM: {response.status_code}, {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the local LLM: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f373d29-2973-4e5e-9343-c8e7b47f0674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the local LLM.\n",
      "LLM Completion Response:  LangChain is a tool designed to streamline and simplify the development of language-based applications using artificial intelligence (AI). It provides developers with pre-built components that can be used to create chatbots, virtual assistants, text analysis tools, and other language-oriented solutions more efficiently.\n",
      "\n",
      "One of the key benefits of LangChain is its modular architecture. Developers can easily integrate different language processing tasks such as natural language understanding (NLU), natural language generation (NLG), sentiment analysis, and entity recognition into their applications\n",
      "Successfully connected to the local LLM for chat.\n",
      "LLM Chat Response: LangChain is an open-source framework designed to simplify the process of building language models and applications. It provides tools and components that enable developers to easily integrate, train, and deploy language models, making it accessible for a wide range of users from researchers to software engineers.\n"
     ]
    }
   ],
   "source": [
    "# Test the completion endpoint\n",
    "prompt = \"Explain the significance of LangChain in AI applications.\"\n",
    "response = get_completion(prompt)\n",
    "if response:\n",
    "    print(f\"LLM Completion Response: {response}\")\n",
    "\n",
    "# Test the chat completion endpoint\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the purpose of LangChain?\"}\n",
    "]\n",
    "chat_response = get_chat_completion(chat_messages)\n",
    "if chat_response:\n",
    "    print(f\"LLM Chat Response: {chat_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de91a4-e5d8-4d60-bec6-19018abe6818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fa6be-b72f-4e66-8f1a-b1775fbb9ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
