{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "model = OpenAI()\n",
    "completion = model.invoke('Hi there!') \n",
    "# Hi!\n",
    "completions = model.batch(['Hi there!', 'Bye!'])\n",
    "# ['Hi!', 'See you!']\n",
    "for token in model.stream('Bye!'):\n",
    "    print(token)\n",
    "    # Good\n",
    "    # bye\n",
    "    # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imperative Composition\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "# the building blocks\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "model = ChatOpenAI()\n",
    "# combine them in a function\n",
    "# @chain decorator adds the same Runnable interface for any function you write\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "# use it\n",
    "result = chatbot.invoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "#Output:\n",
    "#AIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library offer LLMs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream way for the output\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.invoke(prompt):\n",
    "        yield token\n",
    "for part in chatbot.stream({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "}):\n",
    "    print(part)\n",
    "\n",
    "#Output:\n",
    "#AIMessageChunk(content=\"Hugging\")\n",
    "#AIMessageChunk(content=\" Face's\n",
    "#AIMessageChunk(content=\" `transformers`\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for asynchronous execution\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "result = await chatbot.ainvoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "# Output:\n",
    "# > AIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library offer LLMs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarative Composition\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# the building blocks\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "model = ChatOpenAI()\n",
    "# combine them with the | operator\n",
    "chatbot = template | model\n",
    "# use it\n",
    "result = chatbot.invoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "#Output:\n",
    "#AIMessage(content=\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library offer LLMs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crucially, the last line is the same between the two examplesâ€”that is, you use the function and the LCEL sequence in the same way, with invoke/stream/batch.\n",
    "chatbot = template | model\n",
    "for part in chatbot.stream({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "}):\n",
    "    print(part)\n",
    "    # > AIMessageChunk(content=\"Hugging\")\n",
    "    # > AIMessageChunk(content=\" Face's\n",
    "    # > AIMessageChunk(content=\" `transformers`\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = template | model\n",
    "result = await chatbot.ainvoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
