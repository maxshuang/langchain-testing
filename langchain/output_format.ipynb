{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# self-define output json format\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user question along with justification for the answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "\n",
    "#{\n",
    "#  answer: \"They weigh the same\",\n",
    "#  justification: \"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volu\"... 42 more characters\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke(\"apple, banana, cherry\")\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "'''\n",
    "{'properties': {'setup': {'description': 'The setup of the joke',\n",
    "   'title': 'Setup',\n",
    "   'type': 'string'},\n",
    "  'punchline': {'description': 'The punchline to the joke',\n",
    "   'title': 'Punchline',\n",
    "   'type': 'string'}},\n",
    " 'required': ['setup', 'punchline'],\n",
    " 'title': 'Joke',\n",
    " 'type': 'object'}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "model = model.with_structured_output(Joke)\n",
    "\n",
    "model.invoke(\"Tell me a joke about cats\")\n",
    "\n",
    "#{\n",
    "#  setup: \"Why don't cats play poker in the wild?\",\n",
    "#  punchline: \"Too many cheetahs.\"\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "  \"messages\": [\n",
    "    HumanMessage(\"How old was the 30th president of the United States when he died?\")\n",
    "  ]\n",
    "}\n",
    "output = app.astream_events(input, version=\"v2\")\n",
    "\n",
    "# This will emit each word (technically each token) as soon as it is received from the LLM. You can find more details on this pattern here.\n",
    "# https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/\n",
    "async for event in output:\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            print(content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
