{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\",\"Translate this sentence from English to French: I love programming.\"),\n",
    "        (\"ai\", \"J'adore la programmation.\"),\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_history = InMemoryChatMessageHistory()\n",
    "chat_history.add_user_message(\"Translate this sentence from English to French: I love programming.\")\n",
    "chat_history.add_ai_message(\"J'adore la programmation.\")\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\n",
    "    \"messages\": chat_history.messages,\n",
    "})\n",
    "chat_history.add_ai_message(response)\n",
    "\n",
    "input2 = \"What did I just ask you?\"\n",
    "chat_history.add_user_message(input2)\n",
    "\n",
    "chain.invoke({\n",
    "    \"messages\": chat_history.messages,\n",
    "})\n",
    "\n",
    "# AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 61, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5cbb21c2-9c30-4031-8ea8-bfc497989535-0', usage_metadata={'input_tokens': 61, 'output_tokens': 18, 'total_tokens': 79})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic history management\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_history_for_chain = InMemoryChatMessageHistory()\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_history_for_chain,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# the chain we used before\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "\n",
    "# keep track of history for each combo of user_id and conversation_id\n",
    "histories: dict[str, InMemoryChatMessageHistory] = {}\n",
    "def get_session_history(session_id: str = ''):\n",
    "    if session_id not in histories:\n",
    "        histories[session_id] = InMemoryChatMessageHistory()\n",
    "    return histories[session_id]    \n",
    "\n",
    "# chain with history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# in action\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"hi im bob!\"},\n",
    "    config={\"configurable\": {\"sesion_id\": \"123\"}},\n",
    ")\n",
    "# AIMessage(content='Hello Bob! How can I assist you today?')\n",
    "\n",
    "# remembers\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")\n",
    "# AIMessage(content='Your name is Bob. How can I help you today, Bob?')\n",
    "\n",
    "# New session_id --> does not remember\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"whats my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"456\"}},\n",
    ")\n",
    "# AIMessage(content='I'm sorry, but I don't have access to your personal information such as your name. How can I assist you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the last max_tokens in the list of messages by setting a strategy parameter to “last”\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)\n",
    "\n",
    "\n",
    "#[SystemMessage(content=\"you're a good assistant\"),\n",
    "# HumanMessage(content='whats 2 + 2'),\n",
    "# AIMessage(content='4'),\n",
    "# HumanMessage(content='thanks'),\n",
    "# AIMessage(content='no problem!'),\n",
    "# HumanMessage(content='having fun?'),\n",
    "# AIMessage(content='yes!')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "model = ChatOpenAI()\n",
    "# this makes a \"messages\" key available to prompt,\n",
    "# after passing the input messages list through the trimmer \n",
    "chain = {\"messages\": trimmer} | prompt | model\n",
    "# tracking history\n",
    "history = InMemoryChatMessageHistory()\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, lambda: history\n",
    ")\n",
    "# using it\n",
    "with_message_history.invoke(\n",
    "[HumanMessage(content=\"whats my name?\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import ChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# summarize the message to avoid big chat history\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "demo_ephemeral_chat_history.messages\n",
    "[HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
    " AIMessage(content='Hello!'),\n",
    " HumanMessage(content='How are you today?'),\n",
    " AIMessage(content='Fine thanks!')]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "chain = prompt | chat\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | chat\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "    demo_ephemeral_chat_history.add_message(summary_message)\n",
    "    return True\n",
    "\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_summarization.invoke(\n",
    "    {\"input\": \"What did I say my name was?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "### \n",
    "# AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')\n",
    "###\n",
    "# demo_ephemeral_chat_history.messages\n",
    "# [AIMessage(content='The conversation is between Nemo and an AI. Nemo introduces himself and the AI responds with a greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'),\n",
    "# HumanMessage(content='What did I say my name was?'),\n",
    "# AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering messages\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[HumanMessage(content='example input', name='example_user', id='2'),\\n HumanMessage(content='real input', name='bob', id='4'),\\n AIMessage(content='real output', name='alice', id='5')]\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "\"\"\"\n",
    "[SystemMessage(content='you are a good assistant', id='1'),\n",
    "HumanMessage(content='real input', name='bob', id='4'),\n",
    "AIMessage(content='real output', name='alice', id='5')]\n",
    "\"\"\"\n",
    "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])\n",
    "\"\"\"\n",
    "[HumanMessage(content='example input', name='example_user', id='2'),\n",
    " HumanMessage(content='real input', name='bob', id='4'),\n",
    " AIMessage(content='real output', name='alice', id='5')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "chain = filter_ | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging consecutive messages\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage([{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),\n",
    "]\n",
    "\n",
    "merge_message_runs(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "merger = merge_message_runs()\n",
    "chain = merger | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history with retrieval\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./test.txt').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "model = OpenAIEmbeddings()\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(documents, model, connection=connection)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_msg_content(msg):\n",
    "    return msg.content\n",
    "\n",
    "contextualize_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_system_prompt),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "contextualize_chain = (\n",
    "    contextualize_prompt\n",
    "    | ChatOpenAI()\n",
    "    | get_msg_content\n",
    ")\n",
    "\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = (\n",
    "    qa_prompt\n",
    "    | ChatOpenAI()\n",
    "    | get_msg_content\n",
    ")\n",
    " \n",
    "@chain\n",
    "def history_aware_qa(input):\n",
    "     # rephrase question if needed\n",
    "     if input.get('chat_history'):\n",
    "         question = contextualize_chain.invoke(input)\n",
    "     else:\n",
    "         question = input['input']\n",
    "     # get context from retriever\n",
    "     context = retriever.invoke(question)\n",
    "     # get answer\n",
    "     return qa_chain.invoke({\n",
    "         **input,\n",
    "         \"context\": context\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_history_for_chain = InMemoryChatMessageHistory()\n",
    "qa_with_history = RunnableWithMessageHistory(\n",
    "    history_aware_qa,\n",
    "    lambda _: chat_history_for_chain,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "qa_with_history.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting Chat History Long-Term\n",
    "\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "history = PostgresChatMessageHistory(\n",
    "   connection_string='postgresql://langchain:langchain@localhost:6024/langchain',\n",
    "   session_id=\"example\",\n",
    ")\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
