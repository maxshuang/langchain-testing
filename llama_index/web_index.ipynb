{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf748b6-8092-411e-8bf6-0e5c483dec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "import trafilatura\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8b03a4-f98d-4398-9889-2c2e2e5b81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"/Users/maxhuang/projects/llm/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0073a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "llama_load_model_from_file: using device Metal (Apple M3) - 16383 MiB free\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from /Users/maxhuang/projects/llm/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 14B Instruct AWQ\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-AWQ\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q4_0:  337 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 5\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 14B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 14.77 B\n",
      "llm_load_print_meta: model size       = 7.93 GiB (4.61 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 Coder 14B Instruct AWQ\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 242 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/49 layers to GPU\n",
      "llm_load_tensors:  CPU_AARCH64 model buffer size =  7087.50 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8117.45 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.33.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.34.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.34.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.34.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.34.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.34.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.34.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.35.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.35.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.35.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.35.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.35.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.36.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.36.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.37.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.37.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.37.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.37.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.37.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.37.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.37.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.38.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.38.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.38.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.38.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.38.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.38.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.38.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.39.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.39.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.39.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.39.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.39.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.39.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.39.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.40.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.40.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.40.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.40.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.40.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.40.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.40.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.41.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.41.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.42.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.42.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.42.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.42.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.42.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.42.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.42.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.43.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.43.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.43.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.43.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.43.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.43.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.43.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.44.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.44.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.44.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.44.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.44.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.44.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.44.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.45.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.45.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.45.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.45.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.45.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.45.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.45.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.46.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.46.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.46.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.46.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.46.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.46.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.46.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.47.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.47.ffn_up.weight with q4_0_4x8\n",
      "....\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x178828e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x178828720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x17840e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x17882a6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x17840efb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x178905540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x178905930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x144ff20a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x178905dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1789061c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x17882aa30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x178906b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x17840e5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x178907690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x178908a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x178909490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x1789096c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x17890a920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x17890ab50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x144ff2fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x144ff3200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x144ff3c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x17882ac60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x17882bbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x17882c350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x144ff4ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144ff4ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x17890b360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17890bb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x1784106a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x178411180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x178410f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x178411760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x178411ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x17890bf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x178413670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x178413eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1784146c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x17882e0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x17890d560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x17890dda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x17890e570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1784156c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x178416020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x178412e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x17890c730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x17890eeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x17890f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144ff2490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144ff5540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x17890fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x178416760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1784172b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x178417ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1789104f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x178910d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1784182d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x17882d180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x178418a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144ff63d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x17882d9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x17882e420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x178911890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x178911ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x17882e650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17882fbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x178830360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x178418f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144ff70d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144ff6d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144ff8630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x178830820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x178419370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144ff7fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144ff9a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144ff8d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x178419b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x178830c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17841a2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17841b370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17841b630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17841a8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17841aad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x178911530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144ff9490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x178912250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x17841c3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x178912810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x17841d070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x178912a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x178912e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x178831e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x178913550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x178913a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144ffbe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x17841d670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x17841dac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x17841e220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144ffb6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x17841ffe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x178913c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x178914060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144ffc260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144ffd2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x178914bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144ffcac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144ffe310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x178915730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x178915ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144ffe5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x178832a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x16b0844f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144ffdab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144fff730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1788335d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1784213c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x178833a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144fff9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1784203a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144ffec80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144fec3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x16b0855e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x16b0859d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x16b085c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144fec790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x178421650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144fecf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x178421e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144fee7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144fef010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144fedfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x16b087a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x16b0882d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x16b086be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x178834c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1788354d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x16b087410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x16b088530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x16b088920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x178834120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x178835d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1784238d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x178835f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x16b089000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x178422450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x178423120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144feea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x16b089aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x178836d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1784244b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x178424c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x178424ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x178425e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144fefc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144ff0060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x16b089f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x16b08a330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x16b08abc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x16b08b4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x16b08bd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x178837c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x16b08c740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144ff04d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144ff1300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144ff1ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1785043e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x16b08cb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x16b08d790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x178504c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x178505470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x178505c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x178506520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1788380b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1788384a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1784263a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x16b08de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x16b08e670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x16b08eec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x178506fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x178507790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x178507d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x178508680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x178838b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x178509080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x178838f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x178509ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x16b08f910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x1788396f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x16b0900e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x17850a410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x178839c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17850b0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x17850b8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17883a6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x17883b680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x16b090b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x16b090310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x17850a640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x16b091750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x17850c9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x16b092bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x17850d340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x178426910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x178426b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x16b091f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x17850ea90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x17883ae90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x17883cdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x17883d010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x178428550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x17883bc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1784274e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x16b093bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x16b094c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x16b094f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x17883dae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x17883ea80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x16b093fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x17850edc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x17883f720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x17883fec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x17850f210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x16b0959d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1788406f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x178840f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x16b095e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x16b097270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17850fb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x17850ff50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x16b096c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x178510460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x178428f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x178510df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x178512160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x16b097ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x178511400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x178512e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x178841bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x178511a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x178513a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1785142b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x178514b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x178842890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1784294a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x17842b000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x16b099300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x16b099b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x178429c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x17842bca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x178841fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x16b098b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x16b0983c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x178429fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16b099ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x16b09a7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x17842bff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x178842fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x178513330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x16b09afa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x178514e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1785156c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1785159c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x178515cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x178516420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x16b09bf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x178516650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x16b09c720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x178516a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x178517880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x1785188f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x17842cc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x178517c70 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   307.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1686\n",
      "llama_new_context_with_model: graph splits = 98 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.head_count_kv': '8', 'qwen2.embedding_length': '5120', 'qwen2.context_length': '131072', 'general.type': 'model', 'qwen2.attention.head_count': '40', 'general.architecture': 'qwen2', 'qwen2.feed_forward_length': '13824', 'general.basename': 'Qwen2.5-Coder', 'qwen2.block_count': '48', 'general.finetune': 'Instruct-AWQ', 'general.name': 'Qwen2.5 Coder 14B Instruct AWQ', 'general.file_type': '2', 'general.size_label': '14B'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load the local LLM model\n",
    "#llama_model = Llama(model_path=local_model, n_ctx=56000, verbose=False)\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "# Initialize LLM properly using LlamaCPP wrapper\n",
    "llm = LlamaCPP(\n",
    "    model_path=local_model,\n",
    "    model_kwargs={\n",
    "        \"n_ctx\": 2048,\n",
    "        \"n_threads\": 4,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e2a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "# pip install llama-index-embeddings-huggingface\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding_model_path = \"/Users/maxhuang/projects/llm/models/all-MiniLM-L6-v2\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701df9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2329032c-b25b-46df-a2d3-ba964f75615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_website(url, *questions):\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        raise ValueError(f\"Failed to download content from {url}. Please check the URL.\")\n",
    "    else:\n",
    "        print(downloaded)\n",
    "        \n",
    "    text = trafilatura.extract(downloaded)\n",
    "    if not downloaded:\n",
    "        raise ValueError(f\"Failed to download content from {url}. Please check the URL.\")\n",
    "    else: \n",
    "        print(text)\n",
    "    \n",
    "    #print(text)\n",
    "    list_of_documents = [Document(text=text)]\n",
    "    index = VectorStoreIndex.from_documents(list_of_documents)   #.from_texts([text])\n",
    "    engine = index.as_query_engine()\n",
    "    for question in questions:\n",
    "        print(f\"\\n== QUESTION: {question}\\n\")\n",
    "        response = engine.query(question)\n",
    "        print(f\"== RESPONSE: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c1426cd-2d83-4f26-b8a8-de96e69a83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== QUESTION: What instruments does Mark play?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1791 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   63941.91 ms /  1806 tokens\n",
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   391 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22393.71 ms /   439 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== RESPONSE:  Mark plays the guitar, didgeridoo, and American Indian flute. The information is sourced from the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 license, which is applicable to the content provided in the context.\n",
      "\n",
      "== QUESTION: How many books has Mark written?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1791 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   76173.22 ms /  1839 tokens\n",
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   427 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22042.26 ms /   475 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== RESPONSE: 10\n",
      "The query asks for the number of books Mark has written. The context information lists 10 books that Mark has written, including both his older published books and his newer eBooks. Therefore, the answer is 10.\n",
      "\n",
      "== QUESTION: list company names beginning with the letter 'C'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1791 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  115329.86 ms /  2046 tokens\n",
      "llama_perf_context_print:        load time =   61716.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   640 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   63389.01 ms /   895 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== RESPONSE: 1. Capital One\n",
      "2. CastTV\n",
      "3. CompassLabs\n",
      "4. Cognizant\n",
      "5. CastTV\n",
      "6. Cognizant\n",
      "7. CastTV\n",
      "8. Cognizant\n",
      "9. CastTV\n",
      "10. Cognizant\n",
      "11. CastTV\n",
      "12. Cognizant\n",
      "13. CastTV\n",
      "14. Cognizant\n",
      "15. CastTV\n",
      "16. Cognizant\n",
      "17. CastTV\n",
      "18. Cognizant\n",
      "19. CastTV\n",
      "20. Cognizant\n",
      "21. CastTV\n",
      "22. Cognizant\n",
      "23. CastTV\n",
      "24. Cognizant\n",
      "25. CastTV\n",
      "26. Cognizant\n",
      "27. CastTV\n",
      "28. Cognizant\n",
      "29. CastTV\n",
      "30. Cognizant\n",
      "31. CastTV\n",
      "32. Cognizant\n",
      "33. CastTV\n",
      "34. Cognizant\n",
      "35. CastTV\n",
      "36. Cognizant\n",
      "37. CastTV\n",
      "38. Cognizant\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://markwatson.com\"\n",
    "query_website(url, \"What instruments does Mark play?\",\n",
    "                 \"How many books has Mark written?\",\n",
    "                 \"list company names beginning with the letter 'C'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
