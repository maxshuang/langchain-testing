{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff7ca53-c7d3-43e0-9591-0e3f162aa5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load the local LLM model\n",
    "# the gguf format is not compatible with transformers\n",
    "model_path = \"/Users/maxhuang/projects/llm/models/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/llama-3.2-3b-instruct-q8_0.gguf\"  # Replace with your local LLaMA model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9986e1-adbd-4388-8b5c-4e0530d43dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_model():\n",
    "    try:\n",
    "        # Use the transformers pipeline to load the model\n",
    "        hf_pipeline = pipeline(\"text2text-generation\", model=model_path, device=-1)  # Use device=0 for GPU\n",
    "        llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "        print(\"Successfully loaded the local LLM model.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the local LLM model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd604947-5dbb-4275-b944-d10af91d8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(input_text, source_lang, target_lang, llm):\n",
    "    try:\n",
    "        # Construct the prompt for translation\n",
    "        prompt = f\"Translate the following text from {source_lang} to {target_lang}:\\n{input_text}\"\n",
    "        # Generate the response using the LLM\n",
    "        response = llm(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating translation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986073a6-84fb-4241-b668-b7d6dc79bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load the local LLM model: Incorrect path_or_model_id: '/Users/maxhuang/projects/llm/models/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/llama-3.2-3b-instruct-q8_0.gguf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n"
     ]
    }
   ],
   "source": [
    "local_llm = load_local_model()\n",
    "if local_llm:\n",
    "    input_text = \"Hello, how are you?\"\n",
    "    source_language = \"English\"\n",
    "    target_language = \"French\"\n",
    "    translation = translate_text(input_text, source_language, target_language, local_llm)\n",
    "    if translation:\n",
    "        print(f\"Translated Text: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bafb04b-2023-4881-b84b-bbcbf7970c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# Step 1: Specify the path to your GGUF model\n",
    "#model_path = \"/Users/maxhuang/projects/llm/models/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/llama-3.2-3b-instruct-q8_0.gguf\"\n",
    "model_path = \"/Users/maxhuang/projects/llm/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41059835-982c-4420-b95a-e7c7600813c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_model():\n",
    "    try:\n",
    "        # Load the LLM using LlamaCpp\n",
    "        llm = LlamaCpp(model_path=model_path, verbose=False)\n",
    "        print(\"Successfully loaded the local LLM model.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the local LLM model: {e}\")\n",
    "        return None\n",
    "\n",
    "def translate_text(input_text, source_lang, target_lang, llm):\n",
    "    try:\n",
    "        # Construct the prompt for translation\n",
    "        prompt = (\n",
    "            f\"Translate the following text from {source_lang} to {target_lang}:\\n\"\n",
    "            f\"{input_text}\"\n",
    "        )\n",
    "        # Generate the response using the LLM\n",
    "        response = llm(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating translation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a216c26-940f-45de-b2ff-9272c7413e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the local LLM model.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load local model\n",
    "local_llm = load_local_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e97bfa78-9e2e-4936-9384-d29ba67be987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text:  I hope you're doing well. My name is John, and I'm here to help you.\n",
      "你好，你怎么样？我希望你一切都好。我叫约翰，来这里帮你。\n",
      "Here are some tips on how to stay healthy:\n",
      "1. Eat a balanced diet\n",
      "2. Get enough sleep\n",
      "3. Exercise regularly\n",
      "4. Manage stress effectively\n",
      "5. Stay hydrated by drinking plenty of water\n",
      "以下是一些保持健康的小贴士：\n",
      "1. 吃均衡的饮食\n",
      "2. 确保有足够的睡眠\n",
      "3. 定期进行锻炼\n",
      "4. 有效地管理压力\n",
      "5. 通过喝足够的水来保持充分水分摄入\n",
      "Remember, taking care of your health is the most important thing you can do for yourself.\n",
      "请记住，照顾好自己的健康是对自己所能做的最重要的事情。\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test the translation functionality\n",
    "input_text = \"Hello, how are you?\"\n",
    "source_language = \"English\"\n",
    "target_language = \"Chinese\"\n",
    "translation = translate_text(input_text, source_language, target_language, local_llm)\n",
    "if translation:\n",
    "    print(f\"Translated Text: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a6820-4e4c-400e-8932-d9c00c910ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bitnet-cpp",
   "language": "python",
   "name": "bitnet-cpp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
